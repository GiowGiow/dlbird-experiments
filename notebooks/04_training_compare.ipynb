{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2afc5b",
   "metadata": {},
   "source": [
    "# Audio Model Training\n",
    "\n",
    "This notebook trains two audio classification models on Xeno-Canto MFCC features:\n",
    "1. **AudioCNN** - Convolutional neural network for audio\n",
    "2. **AudioViT** - Vision Transformer adapted for audio spectrograms\n",
    "\n",
    "Both models classify bird species from MFCC features extracted from 3-second audio clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80328890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "from src.models.audio_cnn import AudioCNN\n",
    "from src.models.audio_vit import AudioViT\n",
    "from src.datasets.audio import AudioMFCCDataset\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "ARTIFACTS = Path('../artifacts')\n",
    "MODELS_DIR = ARTIFACTS / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device_obj = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = str(device_obj)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc409cc",
   "metadata": {},
   "source": [
    "## Load Data and Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered Xeno-Canto data\n",
    "xc_df = pd.read_parquet(ARTIFACTS / 'xeno_canto_filtered.parquet')\n",
    "\n",
    "# Filter to species with >=2 samples\n",
    "xc_counts = xc_df['species_normalized'].value_counts()\n",
    "species_to_keep = xc_counts[xc_counts >= 2].index\n",
    "xc_df = xc_df[xc_df['species_normalized'].isin(species_to_keep)].copy()\n",
    "\n",
    "# Load splits\n",
    "with open(ARTIFACTS / 'splits' / 'xeno_canto_audio_splits.json', 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "# Create species to label mapping\n",
    "species_list = sorted(xc_df['species_normalized'].unique())\n",
    "species_to_idx = {sp: i for i, sp in enumerate(species_list)}\n",
    "num_classes = len(species_list)\n",
    "\n",
    "print(f\"Dataset: {len(xc_df)} recordings, {num_classes} species\")\n",
    "print(f\"Train: {len(splits['train'])} samples\")\n",
    "print(f\"Val: {len(splits['val'])} samples\")\n",
    "print(f\"Test: {len(splits['test'])} samples\")\n",
    "\n",
    "# Create datasets\n",
    "cache_dir = ARTIFACTS / 'audio_mfcc_cache' / 'xeno_canto'\n",
    "\n",
    "train_dataset = AudioMFCCDataset(\n",
    "    df=xc_df,\n",
    "    cache_dir=cache_dir,\n",
    "    indices=splits['train'],\n",
    "    species_to_idx=species_to_idx,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "val_dataset = AudioMFCCDataset(\n",
    "    df=xc_df,\n",
    "    cache_dir=cache_dir,\n",
    "    indices=splits['val'],\n",
    "    species_to_idx=species_to_idx,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "test_dataset = AudioMFCCDataset(\n",
    "    df=xc_df,\n",
    "    cache_dir=cache_dir,\n",
    "    indices=splits['test'],\n",
    "    species_to_idx=species_to_idx,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloader batches:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_loader)} batches\")\n",
    "print(f\"  Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac704b",
   "metadata": {},
   "source": [
    "## Train AudioCNN\n",
    "\n",
    "Convolutional neural network designed for audio MFCC features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56445f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AudioCNN\n",
    "model = AudioCNN(num_classes=num_classes).to(device_obj)\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    checkpoint_dir=MODELS_DIR / 'audio_cnn',\n",
    "    experiment_name='AudioCNN',\n",
    "    use_amp=True,\n",
    "    gradient_clip=1.0,\n",
    "    early_stopping_patience=7\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Starting AudioCNN training...\")\n",
    "print(\"This may take 20-40 minutes depending on your GPU.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08796bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = trainer.train(num_epochs=50)\n",
    "\n",
    "print(f\"\\nâœ“ AudioCNN training complete\")\n",
    "print(f\"âœ“ Best val accuracy: {max(history['val_acc']):.4f}\")\n",
    "print(f\"âœ“ Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"âœ“ Final val loss: {history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('AudioCNN - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('AudioCNN - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'audio_cnn' / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save history\n",
    "with open(MODELS_DIR / 'audio_cnn' / 'history.json', 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved training curves and history to {MODELS_DIR / 'audio_cnn'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b1c67",
   "metadata": {},
   "source": [
    "## Train AudioViT\n",
    "\n",
    "Vision Transformer adapted for audio spectrograms (MFCC features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AudioViT\n",
    "model = AudioViT(num_classes=num_classes, pretrained='google/vit-base-patch16-224').to(device_obj)\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Setup optimizer and scheduler (AdamW + Cosine annealing for ViT)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=50, eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    checkpoint_dir=MODELS_DIR / 'audio_vit',\n",
    "    experiment_name='AudioViT',\n",
    "    use_amp=True,\n",
    "    gradient_clip=1.0,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Starting AudioViT training...\")\n",
    "print(\"This may take 40-80 minutes depending on your GPU.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eda936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "vit_history = trainer.train(num_epochs=50)\n",
    "\n",
    "print(f\"\\nâœ“ AudioViT training complete\")\n",
    "print(f\"âœ“ Best val accuracy: {max(vit_history['val_acc']):.4f}\")\n",
    "print(f\"âœ“ Final train loss: {vit_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"âœ“ Final val loss: {vit_history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for AudioViT\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(vit_history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(vit_history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('AudioViT - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(vit_history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(vit_history['val_acc'], label='Val', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('AudioViT - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'audio_vit' / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save history\n",
    "with open(MODELS_DIR / 'audio_vit' / 'history.json', 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in vit_history.items()}, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved training curves and history to {MODELS_DIR / 'audio_vit'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401750c8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Both audio models have been trained and their checkpoints saved. The models can now be evaluated on the test set."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
