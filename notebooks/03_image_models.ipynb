{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6107fa13",
   "metadata": {},
   "source": [
    "# Image Model Training\n",
    "\n",
    "This notebook trains two image classification models on the CUB-200-2011 dataset:\n",
    "1. **ResNet-18** - CNN-based architecture with residual connections\n",
    "2. **ViT-B/16** - Vision Transformer with self-attention\n",
    "\n",
    "Both models are trained on 90 species from the intersection of Xeno-Canto and CUB datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26111e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "from src.models.image_resnet import ImageResNet\n",
    "from src.models.image_vit import ImageViT\n",
    "from src.datasets.image import ImageDataset, get_image_transforms\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "ARTIFACTS = Path('../artifacts')\n",
    "MODELS_DIR = ARTIFACTS / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device_obj = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = str(device_obj)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9b2f8",
   "metadata": {},
   "source": [
    "## Load Data and Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered CUB data\n",
    "cub_df = pd.read_parquet(ARTIFACTS / 'cub_filtered.parquet')\n",
    "\n",
    "# Filter to species with >=2 samples (needed for stratification)\n",
    "cub_counts = cub_df['species_normalized'].value_counts()\n",
    "species_to_keep = cub_counts[cub_counts >= 2].index\n",
    "cub_df = cub_df[cub_df['species_normalized'].isin(species_to_keep)].copy()\n",
    "\n",
    "# Load splits\n",
    "with open(ARTIFACTS / 'splits' / 'cub_image_splits.json', 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "# Create species to label mapping\n",
    "species_list = sorted(cub_df['species_normalized'].unique())\n",
    "species_to_idx = {sp: i for i, sp in enumerate(species_list)}\n",
    "num_classes = len(species_list)\n",
    "\n",
    "print(f\"Dataset: {len(cub_df)} images, {num_classes} species\")\n",
    "print(f\"Train: {len(splits['train'])} samples\")\n",
    "print(f\"Val: {len(splits['val'])} samples\")\n",
    "print(f\"Test: {len(splits['test'])} samples\")\n",
    "\n",
    "# Show species distribution\n",
    "print(f\"\\nSample species:\")\n",
    "for sp in species_list[:5]:\n",
    "    count = (cub_df['species_normalized'] == sp).sum()\n",
    "    print(f\"  {sp}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc92485",
   "metadata": {},
   "source": [
    "## Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1766ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with transforms\n",
    "train_dataset = ImageDataset(\n",
    "    df=cub_df,\n",
    "    indices=splits['train'],\n",
    "    species_to_idx=species_to_idx,\n",
    "    transform=get_image_transforms(train=True, image_size=224)\n",
    ")\n",
    "\n",
    "val_dataset = ImageDataset(\n",
    "    df=cub_df,\n",
    "    indices=splits['val'],\n",
    "    species_to_idx=species_to_idx,\n",
    "    transform=get_image_transforms(train=False, image_size=224)\n",
    ")\n",
    "\n",
    "test_dataset = ImageDataset(\n",
    "    df=cub_df,\n",
    "    indices=splits['test'],\n",
    "    species_to_idx=species_to_idx,\n",
    "    transform=get_image_transforms(train=False, image_size=224)\n",
    ")\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, \n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloader batches:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_loader)} batches\")\n",
    "print(f\"  Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f75777",
   "metadata": {},
   "source": [
    "## Train ResNet-18\n",
    "\n",
    "ResNet-18 is a convolutional neural network with 18 layers and skip connections. We use transfer learning with ImageNet pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ResNet-18\n",
    "model = ImageResNet(num_classes=num_classes, pretrained=True).to(device_obj)\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    checkpoint_dir=MODELS_DIR / 'image_resnet18',\n",
    "    experiment_name='ImageResNet18',\n",
    "    use_amp=True,\n",
    "    gradient_clip=1.0,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Starting ResNet-18 training...\")\n",
    "print(\"This may take 30-60 minutes depending on your GPU.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = trainer.train(num_epochs=50)\n",
    "\n",
    "print(f\"\\nâœ“ ResNet-18 training complete\")\n",
    "print(f\"âœ“ Best val accuracy: {max(history['val_acc']):.4f}\")\n",
    "print(f\"âœ“ Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"âœ“ Final val loss: {history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('ResNet-18 - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('ResNet-18 - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'image_resnet18' / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save history\n",
    "with open(MODELS_DIR / 'image_resnet18' / 'history.json', 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f, indent=2)\n",
    "    \n",
    "print(f\"âœ“ Saved training curves and history to {MODELS_DIR / 'image_resnet18'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfe83d",
   "metadata": {},
   "source": [
    "## Train Vision Transformer (ViT-B/16)\n",
    "\n",
    "Vision Transformer applies self-attention mechanisms to image patches. We use the base model with 16x16 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335de5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ViT-B/16\n",
    "model = ImageViT(num_classes=num_classes, pretrained='google/vit-base-patch16-224').to(device_obj)\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Setup optimizer and scheduler (AdamW + Cosine annealing for ViT)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=50, eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    checkpoint_dir=MODELS_DIR / 'image_vit',\n",
    "    experiment_name='ImageViT',\n",
    "    use_amp=True,\n",
    "    gradient_clip=1.0,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Starting ViT-B/16 training...\")\n",
    "print(\"This may take 45-90 minutes depending on your GPU.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "vit_history = trainer.train(num_epochs=50)\n",
    "\n",
    "print(f\"\\nâœ“ ViT-B/16 training complete\")\n",
    "print(f\"âœ“ Best val accuracy: {max(vit_history['val_acc']):.4f}\")\n",
    "print(f\"âœ“ Final train loss: {vit_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"âœ“ Final val loss: {vit_history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(vit_history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(vit_history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('ViT-B/16 - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(vit_history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(vit_history['val_acc'], label='Val', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('ViT-B/16 - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'image_vit' / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save history\n",
    "with open(MODELS_DIR / 'image_vit' / 'history.json', 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in vit_history.items()}, f, indent=2)\n",
    "    \n",
    "print(f\"âœ“ Saved training curves and history to {MODELS_DIR / 'image_vit'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171aac93",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Both image models have been trained and their checkpoints saved. The models can now be evaluated on the test set."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
