%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Comparative Analysis of Visual and Acoustic Deep Learning Models for Bird Classification}

\begin{document}

\twocolumn[
\icmltitle{Comparative Analysis of Visual and Acoustic Deep Learning Models for Bird Classification}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Anne Carvalho}{equal,ufmg}
\icmlauthor{Giovanni Martinelli}{equal,ufmg}
\icmlauthor{Lucas Andrade}{equal,ufmg}
\icmlauthor{Victor Augusto L. Cruz}{equal,ufmg}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{ufmg}{Department of Computer Science, Universidade Federal de Minas Gerais, 31270-901, Belo Horizonte, MG, Brazil.}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\icmlcorrespondingauthor{Giovanni Martinelli}{giovanni@ufmg.br}
%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\newcommand{\mesquita}[1]{\textcolor{red}{  \textbf{Mesquita:} #1}}

\begin{abstract}
 The accurate identification of birds and other morphologically similar animals is essential in biodiversity monitoring and conservation contexts, as it underpins processes such as population estimation, geographic distribution mapping, and the detection of population declines. Misclassification can compromise ecological assessments and negatively influence management and conservation policies. Moreover, properly distinguishing between species with similar appearances is crucial for mitigating ecological impacts, since visually nearly identical species may play very different roles in the ecosystem, acting, for example, as disease vectors, invasive species, or key native species. Therefore, in this work, different methodologies for bird species identification are explored, aiming to uncover the best attributes for such tasks. The findings presented here demonstrate the superiority of Transformer-based architectures in both visual and acoustic domains. Specifically, Vision Transformers (ViT) achieved 92.33\% accuracy on the audiovisual intersection subset, while Audio Spectrogram Transformers (AST) using Log-Mel Spectrograms reached 57.28\%, significantly outperforming convolutional baselines and demonstrating the efficacy of attention mechanisms for fine-grained categorization. The code for this project is available at \url{https://github.com/GiowGiow/dlbird-experiments}.
\end{abstract}

\section{Introduction}
\label{introduction}
Image classification is a central task in the field of computer vision, being defined as the process of assigning a categorical label to an input image based on its visual properties and the spatial patterns it contains. In the early stages of the field, this process relied mainly on handcrafted feature extraction methods, such as Histogram of Oriented Gradients (HOG) and Scale-Invariant Feature Transform (SIFT). Although effective in a variety of scenarios, such methods exhibited limitations stemming from their rigid and heuristic-driven nature and their limited ability to generalize to complex variations in images.

The advent of Convolutional Neural Networks (CNNs) introduced a significant paradigm shift by enabling discriminative representations to be learned directly from data. This approach allowed models to learn progressively more abstract and hierarchically structured features, resulting in substantial performance gains. Architectures such as ResNet \cite{he2016deep}, DenseNet, and EfficientNet have become established state-of-the-art references for classification tasks in recent years. More recently, however, Vision Transformers (ViT) have gained prominence by adapting self-attention mechanisms—originally developed for Natural Language Processing—to the vision domain. These architectures have demonstrated performance competitive with traditional CNNs, especially in large-scale settings.

The continuous advancement of these models has enabled the application of deep learning techniques in specialized domains, such as bird species classification, the central theme of this work. Automatic bird identification plays a key role in ecological and biological studies, contributing to several tasks, such as ecosystem monitoring, biodiversity conservation, and the mitigation of illegal hunting practices. However, bird classification presents challenges that differ substantially from those encountered in conventional image classification tasks. In particular, biodiversity datasets often exhibit severe class imbalance due to the greater abundance of common species compared to the limited availability of images for rare species. Furthermore, the task is characterized as a Fine-Grained Image Classification problem, marked by high intra-class variability—stemming from variations in pose, illumination, plumage, and occlusions—and low inter-class variability, as morphological differences between distinct species may be visually subtle. These factors require models capable of capturing extremely fine visual nuances while remaining robust to significant internal variations.

Furthermore, biological classification offers unique multimodal opportunities; specifically, vocalizations provide a distinct signal orthogonal to visual morphology. So, in this research we want to try and use both the visual aspect and the auditory aspect on the Fine-Grained Image Classification problem to try and identify what are the more defining features on birds on the computational point of view.
\section{Related Work}
\label{relatedwork}
Several studies have been proposed to address the challenges of bird species classification.
\citet{10.1145/3348445.3348480}, in Bird Species Classification from an Image Using VGG-16 Network, employ transfer learning using the VGG-16 architecture as a feature extractor. From the resulting 4096-dimensional feature vectors, different supervised classifiers such as Support Vector Machine (SVM), K-Nearest Neighbor (KNN), and Random Forest (RF) were evaluated. The linear-kernel SVM achieved the best performance, reaching an accuracy of 89\%.

\citet{9862344}, in Bird Species Classification Using Deep Learning, adopt an approach based on Deep Convolutional Neural Networks, aiming to capture distinctive visual features such as beak shape, plumage patterns, and head region. Using the CUB-200-2011 dataset, their method achieved 89\% accuracy, surpassing earlier benchmark models such as Pose Normalization (82\%) and Multiple-Granularity CNN (83\%).

In another direction, \citet{6722493}, in Bird Species Classification Based on Color Features, investigate an approach based exclusively on color information, applying chromatic segmentation to isolate the background and constructing histograms in RGB and HSV color spaces. A multiclass SVM was used for the classification step, resulting in a maximum accuracy of 8.60\%, showing that color features alone are insufficient to distinguish a large number of species.

Finally, Kumar et al. \cite{das2018birdspeciesclassificationusing}, in Bird Species Classification Using Transfer Learning with Multistage Training, propose a multi-stage strategy combining detection and classification. First, a Mask R-CNN pre-trained on the COCO dataset is used to localize regions of interest containing the birds. Next, an ensemble composed of Inception ResNet V2 and Inception V3 is trained in two phases: (i) using the original images and (ii) using cropped images, with the goal of capturing both macro- and micro-structural features. On the Indian bird dataset, the method achieved an F1-score of 55.67\%.

In the domain of audio classification, BirdNET \cite{kahl2021birdnet} stands as the industry standard for avian diversity monitoring, utilizing a custom ResNet architecture to identify over 3,000 species from their vocalizations. More recently, Google's Perch \cite{ghani2023global} has pushed the state-of-the-art by employing efficient Transformer-based models for global-scale bird species recognition in real-time.

Regarding Fine-Grained Visual Classification (FGVC) specifically for the CUB-200 dataset, specialized architectures have been proposed to handle high intra-class variance. NTS-Net \cite{yang2018learning} introduces a Navigator-Teacher-Scrutinizer network to automatically locate informative regions without bounding box annotations. Similarly, TransFG \cite{he2022transfg} adapts the Vision Transformer architecture to capture subtle discriminative features by selecting top-k attention patches, demonstrating superior performance on fine-grained benchmarks.

\section{Methodology}
\label{methodology}

\subsection{Datasets Characterization}

This work explores bird classification using the popular CUB 200 Dataset \cite{WahCUB_200_2011}, which covers 200 bird species in a total of 11,788 images. The dataset is notable for its richness in annotations, including 312 visual attributes per image, and for the high variability of the captures (different angles and scenarios). Despite its high quality and completeness, the limited number of images per class (around 60) represents a constraint for classification architectures that require large volumes of data. For processing, all images, originally of varying resolutions, were resized to 224x224 pixels.

For audio classification, we constructed a dataset from Xeno-Canto \cite{noauthor_xeno-canto_nodate} containing recordings for the species present in CUB-200. To enable multimodal analysis, we specifically focused on the intersection of these two datasets, resulting in a subset of 90 bird species common to both CUB-200 and Xeno-Canto. This intersection dataset allows for direct comparison and future fusion of visual and acoustic modalities.

Figure \ref{fig:class_imbalance} illustrates the distribution of audio samples across the intersection dataset. The disparity is driven by the nature of citizen science data collection: synanthropic species (those living near humans) like the House Sparrow are overrepresented, while elusive or migratory species like the Hooded Merganser have scarce recordings.

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{class_distribution.pdf}}
\caption{Log-scale distribution of sample counts per species in the raw intersection dataset. The "long-tail" imbalance (1216:1 ratio) is clearly visible. For training, species with fewer than 2 samples were filtered, reducing the effective imbalance to 608:1.}
\label{fig:class_imbalance}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Image Classification}

For the task of image classification in the context of the CUB-200-2011 dataset, the objective is to analyze the difficulties inherent to Fine-Grained Visual Classification (FGVC) and to understand the limitations and effectiveness of different architectures for bird-species identification. The methodology was divided into three main approaches.

\paragraph{Attribute-Based Visual Classification (Traditional Approach)} The first stage explored classification using exclusively the 312 visual attributes provided by the dataset. These attributes explicitly encode morphological and phenotypic properties such as beak morphology, feather coloration and patterns, tail shape, and eye color.

Gaussian Naive Bayes was first employed as a probabilistic baseline \cite{hand2001idiot}. This model assumes conditional independence among features and models each attribute using a class-specific Gaussian distribution. Although this assumption is rarely true in fine-grained visual tasks—where attributes exhibit strong correlation—it provides a lightweight generative baseline and serves as a lower bound for performance.

K-Nearest Neighbors (KNN) was evaluated to assess instance-based classification in the attribute space \cite{cover1967nearest}. Using k=5, the model assigns each sample the label shared by the majority of its nearest neighbors under Euclidean distance. KNN is sensitive to local geometry and therefore helpful for examining whether species clusters are separable based purely on attribute vectors.

Decision Trees and Random Forests were included to investigate non-linear decision boundaries. A Decision Tree with maximum depth 10 restricts memorization and promotes generalizable rules based on attribute thresholds \cite{breiman1984classification}. Random Forests extend this idea by aggregating 300 randomized trees, reducing variance and improving robustness to noisy or redundant attributes, which are common in manually annotated datasets \cite{breiman2001random}.

Linear Discriminant Analysis (LDA) was used to measure how well linear projections separate bird species given their semantic attributes \cite{fisher1936use}. By maximizing between-class variance while minimizing within-class variance, LDA tests whether attribute combinations form linearly discriminative subspaces—a property relevant for FGVC, where subtle attribute differences often encode species-specific traits.

Support Vector Machines (SVMs) were then applied as margin-based classifiers, using class-weight balancing to mitigate mild imbalance in attribute distributions \cite{cortes1995support}. The SVM seeks the hyperplane that maximizes the decision margin, enabling it to capture fine-grained distinctions between species encoded in the attributes. Probability calibration was enabled to support later comparative evaluation.

Finally, Logistic Regression with the lbfgs solver and 1000 iterations provided a linear probabilistic baseline \cite{cox1958regression}. By modeling class likelihoods through a softmax function over weighted attribute combinations, it evaluates whether the semantic attributes contain linearly separable information across all 200 species.

Each model was evaluated using 5-fold cross-validation, and results were compared in terms of accuracy and F1-score. Among these, SVM achieved the best performance (47\% accuracy), confirming its ability to capture complex decision boundaries, though still limited compared to deep learning approaches.

\paragraph{Classification with Convolutional Neural Networks (CNNs)} The second stage explored deep vision architectures, specifically EfficientNetB0 \cite{tan2019efficientnet}, initialized with ImageNet-pretrained weights.

Because of limitations in processing time and computational resources, we adopted a fine-tuning strategy that freezes the initial convolutional layers and trains only the upper layers and classification head. This allowed us to evaluate how effectively ImageNet knowledge transfers to the fine-grained bird-classification domain.

For EfficientNetB0, the base network was loaded without the fully connected head and using global average pooling. Fine-tuning was conducted by unfreezing only the top 25 layers, while the earlier convolutional layers remained frozen to preserve low-level pretrained representations. The classification head consisted of a Batch Normalization layer, followed by Dropout (0.25), a dense projection of 512 units with ReLU activation, an additional Dropout layer (0.25), and a final softmax output. 

The model was trained using the Adam optimizer, early stopping based on validation accuracy with a patience of five epochs, and a maximum of 20 training epochs. The data were split into 80\% training, 10\% validation, and 10\% testing.

\paragraph{Classification with Vision Transformers (ViT)} Finally, models from the Vision Transformer (ViT) family \cite{dosovitskiy2020image} were employed, representing state-of-the-art self-attention–based architectures in computer vision. We specifically utilized the \texttt{vit-base-patch16-224} architecture. This model divides the 224x224 input image into a sequence of 16x16 patches, which are linearly projected and processed by a stack of Transformer encoders. Unlike CNNs, ViT relies on self-attention mechanisms to capture global dependencies across the entire image.

Two variants were evaluated: one pretrained on ImageNet-1k and another on ImageNet-21k, the latter providing substantially broader semantic coverage. Unlike CNNs, Vision Transformers rely less on built-in inductive biases—such as locality and translation equivariance—and therefore depend more heavily on either large-scale training datasets or strong data-augmentation schemes to achieve high generalization. Empirically, preliminary experiments indicated that ViT performance improved noticeably when exposed to more diverse training data, motivating the evaluation of multiple augmentation regimes.

The first regime used no data augmentation, applying only resizing and normalization, serving as a minimal baseline. The second regime incorporated MixUp and CutMix \cite{yun2019cutmix}, two data-mixing techniques that blend images and labels either linearly or through spatial masking. These methods act as strong regularizers by preventing the model from overfitting to spurious fine-grained details and encouraging learning of more global, robust features. The third regime combined CutMix, MixUp, and RandAugment \cite{cubuk2020randaugment}, adding stochastic geometric and color transformations—such as rotations, elastic distortions, and brightness or contrast perturbations—to further increase synthetic diversity in the training set.

These configurations enabled the investigation of modern regularization strategies and the benefits of large-scale pretraining for improving fine-grained species-classification accuracy.

\subsection{Audio Classification}

To complement the visual analysis, we investigate audio-based species identification using bird vocalizations. This modality presents unique challenges, including variable signal duration, background noise, and the need for effective time-frequency representations.

\paragraph{Audio Feature Engineering}
Initial experiments employed Mel-Frequency Cepstral Coefficients (MFCCs) \cite{davis1980comparison}, a standard feature in speech recognition. We constructed a 3-channel input tensor consisting of static MFCCs stacked with their first and second derivatives (Delta and Delta-Delta) to capture temporal dynamics \cite{abdel2014convolutional}. However, this representation yielded suboptimal performance (approximately 38\% accuracy). We observed severe training instability and convergence issues, partly attributed to the difficulty in effectively normalizing these high-variance coefficients across a diverse dataset. Furthermore, MFCCs decorrelate the signal, removing spectro-temporal structures that Convolutional Neural Networks (CNNs) and Transformers exploit effectively.

Consequently, we adopted Log-Mel Spectrograms (LMS) as the primary input representation for our best performing model (AST). LMS preserves the time-frequency locality of the signal, treating audio as an image-like tensor suitable for 2D architectures. We processed audio clips with a sampling rate ($F_s$) of 22.05 kHz. Spectrograms were generated using a Fast Fourier Transform (FFT) window size ($n\_fft$) of 2048, a hop length of 512, and 128 Mel filter banks ($n\_mels=128$). The resulting spectrograms were padded or truncated to a fixed length of 1024 frames, resulting in a 128x1024 input tensor. This configuration captures sufficient frequency resolution for bird calls while maintaining temporal precision.

\paragraph{Network Architectures}
We evaluated three distinct architectures adapted for audio classification, corresponding to the phases of our experimentation:

\textit{Phase 0: AudioViT (Baseline):} To establish a baseline, we applied a standard Vision Transformer (ViT-Base) directly to MFCC features. The MFCCs were resized to 224x224 to match the expected input of the pretrained ViT. This naive approach served to test the transferability of visual models to cepstral audio representations.

\textit{Phase 2: AudioCNNv2:} We utilized a 14-layer Convolutional Neural Network inspired by the PANNs architecture \cite{kong2020panns}. This model processes MFCCs using a hierarchy of convolutional blocks with increasing channel width (up to 512), designed to capture local spectro-temporal patterns. It was trained using Focal Loss to handle class imbalance.

\textit{Phase 3: Audio Spectrogram Transformer (AST):} Our final and most advanced model, AST \cite{gong21b_interspeech}, is a specialized Transformer for audio. It takes Log-Mel Spectrograms (128x1024) as input, splits them into 16x16 patches, and applies self-attention. AST is initialized with ImageNet weights and further pretrained on AudioSet, allowing it to effectively capture long-range dependencies in bird calls.

\paragraph{Class Imbalance Mitigation}
The raw dataset exhibits severe class imbalance, with a ratio of 1216:1 between the most and least common classes. However, for our experiments, we filtered out species with fewer than 2 samples, resulting in an effective imbalance of 608:1. To address this, we implemented two key strategies:

\textit{Focal Loss:} We replaced the standard Cross-Entropy loss with Focal Loss \cite{lin2017focal}, defined as $FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)$. By setting the focusing parameter $\gamma=2.0$, the loss function down-weights easy examples and focuses training on hard, misclassified examples, preventing the vast number of easy negatives from overwhelming the gradient.

\textit{Advanced Augmentation:} We applied SpecAugment \cite{park19e_interspeech}, which masks blocks of time and frequency channels in the spectrogram, forcing the model to be robust to partial signal loss. Additionally, we used MixUp \cite{zhang2018mixup}, which trains the model on convex combinations of pairs of examples and their labels, smoothing the decision boundaries and improving generalization for rare classes.

\subsection{Experimental Setup for Intersection Analysis}
For the comparative analysis on the intersection dataset (90 species), we adopted a unified experimental protocol to ensure fair comparison between visual and acoustic models.

\paragraph{Data Splits}
The intersection dataset was partitioned using a stratified split strategy to preserve the class distribution across subsets. We used 70\% of the data for training, 15\% for validation, and 15\% for testing. This differs slightly from the full CUB-200 splits to accommodate the smaller sample size of the intersection subset while ensuring sufficient validation data.

\paragraph{Training Configuration}
All models were trained for a maximum of 50 epochs with early stopping (patience of 10 epochs) based on validation loss. We used a batch size of 32 and Automatic Mixed Precision (AMP) to optimize training efficiency.
Optimization strategies were tailored to each architecture:
\begin{itemize}
    \item \textbf{Image ResNet-18:} Trained with SGD (learning rate $10^{-2}$, momentum 0.9, weight decay $10^{-4}$) and a StepLR scheduler (decay by 0.1 every 10 epochs).
    \item \textbf{Image ViT:} Trained with AdamW (learning rate $10^{-4}$, weight decay $10^{-2}$) and a Cosine Annealing scheduler ($T_{max}=50$).
    \item \textbf{Audio CNN:} Trained with Adam (learning rate $10^{-3}$).
    \item \textbf{Audio ViT:} Trained with Adam (learning rate $10^{-4}$) and a Cosine Annealing scheduler.
\end{itemize}

\section{Results}
\label{results}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{model_comparison_chart.pdf}}
\caption{Comparative performance of visual and acoustic models. Left: Accuracy on the full CUB-200 dataset for image models. Right: Accuracy on the intersection dataset for audio and multimodal baselines. Vision Transformers (ViT) dominate both tasks, while AST significantly outperforms MFCC-based audio baselines.}
\label{fig:model_comparison}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Image Classification}

For each classification method developed in this study, we present below the corresponding evaluation metrics on the full CUB-200-2011 dataset (200 classes).

\subsubsection{Traditional Methods}
For the attribute-based traditional classifiers, the SVM achieved the best performance, obtaining an average accuracy of approximately 0.47 and a similar average F1-score under 5-fold cross-validation. This outcome was expected, given the SVM’s ability to capture complex decision boundaries in high-dimensional spaces. Nevertheless, there remains substantial room for improvement, especially considering the inherent limitations of manually curated attributes in capturing fine-grained visual variability.

\begin{table}[ht]
\centering
\caption{Traditional vs. Deep Learning Baselines (CUB-200)}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{F1 Macro} \\
\midrule
SVM            & 0.471 $\pm$ 0.003 & 0.475 $\pm$ 0.002 \\
LDA            & 0.467 $\pm$ 0.009 & 0.462 $\pm$ 0.007 \\
Logistic       & 0.454 $\pm$ 0.012 & 0.460 $\pm$ 0.010 \\
Random Forest  & 0.474 $\pm$ 0.003 & 0.453 $\pm$ 0.003 \\
KNN (k=5)      & 0.300 $\pm$ 0.010 & 0.292 $\pm$ 0.008 \\
Naive Bayes    & 0.188 $\pm$ 0.004 & 0.174 $\pm$ 0.006 \\
Decision Tree  & 0.156 $\pm$ 0.002 & 0.143 $\pm$ 0.002 \\
\midrule
EfficientNetB0 & 0.74 & 0.74 \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Convolutional Neural Networks}

In the CNN-based experiments on the full CUB-200 dataset, we evaluated EfficientNetB0. EfficientNetB0 demonstrated strong performance (74\% accuracy). For EfficientNetB0, the base network was loaded without the fully connected head and using global average pooling. Fine-tuning was conducted by unfreezing only the top 25 layers, while the earlier convolutional layers remained frozen to preserve low-level pretrained representations. The classification head consisted of a Batch Normalization layer, followed by Dropout (0.25), a dense projection of 512 units with ReLU activation, an additional Dropout layer (0.25), and a final softmax output.



\subsubsection{Vision Transformers}

Finally, the Vision Transformer models achieved the strongest results among all evaluated methods. Both models outperformed the previous approaches. Surprisingly, the google/vit-base-patch16-224 model pretrained on ImageNet-1k consistently outperformed google/vit-base-patch16-224-in21k, despite the latter having been pretrained on a much larger and semantically richer dataset. This result contradicts standard literature benchmarks, which typically show benefits from larger pretraining datasets for fine-grained tasks \cite{kornblith2019better}. We hypothesize that the ImageNet-21k weights, being more generalized, required a longer warm-up phase or a smaller learning rate to adapt to the specific fine-grained features of CUB-200, whereas the ImageNet-1k weights were already sharper for object-centric classification.

Another unexpected finding was that advanced data-augmentation techniques (MixUp, CutMix, and RandAugment) did not improve performance, even though Vision Transformers are known to benefit from larger and more diverse training distributions. We observed that strong regularization (MixUp) harmed performance (85\% $\to$ 81\%). We attribute this to the high inter-class similarity in fine-grained tasks; blending images of nearly identical species likely confuses the model by smoothing decision boundaries that need to be extremely sharp. Still, the obtained results are acceptable for a fine-grained species-identification task, and indicate that further hyperparameter optimization could yield additional gains.

\begin{table}[ht]
\centering
\caption{Performance Comparison Across Augmentations. Note that the baseline (None) performed best.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|cc|cc|cc}
\hline
\textbf{Model} &
\multicolumn{2}{c|}{\textbf{None}} &
\multicolumn{2}{c|}{\textbf{Mix+Cut}} &
\multicolumn{2}{c}{\textbf{Mix+Cut+Rand}} \\
\hline
 & \textbf{F1} & \textbf{Acc}
 & \textbf{F1} & \textbf{Acc}
 & \textbf{F1} & \textbf{Acc} \\
\hline
\textbf{ViT-Base (1k)} &
\textbf{0.85} & \textbf{0.85} &
0.81 & 0.81 &
-- & -- \\
\textbf{ViT-Base (21k)} &
0.83 & 0.83 &
0.80 & 0.80 &
0.83 & 0.83 \\
\hline
\end{tabular}%
}
\end{table}

\begin{table}[ht]
\centering
\caption{Summary of Best Models by Category}
\label{tab:summary_performance}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llcc}
\toprule
\textbf{Category} & \textbf{Best Model} & \textbf{Accuracy} & \textbf{Dataset} \\
\midrule
Traditional & SVM & 0.47 & CUB-200 \\
CNN & EfficientNetB0 & 0.74 & CUB-200 \\
Transformer & ViT-Base & 0.92 & Intersection \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsubsection{Image Classification on Intersection Dataset}
To establish a fair baseline for our audiovisual experiments, we also evaluated our image models on the 90-species intersection dataset. We trained a ResNet-18 model specifically for this subset. While both models utilized ImageNet pretrained weights, the fine-tuning strategy differed slightly to accommodate the smaller dataset size. For ResNet-18, we unfroze the final fully connected layer and the last convolutional block, whereas the EfficientNetB0 approach involved unfreezing the top 25 layers. This model achieved an accuracy of 85.52\%. On this same subset, the ViT-B/16 model achieved a test accuracy of 92.33\%, significantly outperforming both the ResNet-18 baseline and the audio-only models. This high performance confirms that visual features remain the dominant modality for bird identification, though audio provides complementary information for specific cases.

\subsection{Audio Classification Results}

We present the progression of our audio classification models in Table \ref{tab:audio_results}.

\begin{table}[ht]
\centering
\caption{Audio Classification Performance (90-Species Intersection)}
\label{tab:audio_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Phase / Model} & \textbf{Features} & \textbf{Strategy} & \textbf{Accuracy} & \textbf{F1 Macro} \\
\midrule
Phase 0: AudioViT & MFCC & Baseline & 37.79\% & 0.13 \\
Phase 2: AudioCNNv2 & MFCC & Focal Loss & 42.72\% & 0.22 \\
Phase 3: AST & LMS & Transfer Learning & \textbf{57.28\%} & \textbf{0.36} \\
\bottomrule
\end{tabular}%
}
\end{table}

\paragraph{Analysis}
Our baseline AudioViT using MFCCs achieved only 37.79\% accuracy. Notably, this experiment demonstrated that direct transfer learning from ImageNet-pretrained Vision Transformers to MFCC tensors failed to prove useful, performing worse than a simple Convolutional Neural Network (AudioCNNv2) which achieved 42.72\%. Examination of the training dynamics revealed severe and early overfitting: while training accuracy surged to nearly 99\% within just 10 epochs, validation loss began to diverge and increase as early as epoch 4. This indicates that the model simply memorized the training set noise rather than learning generalizable spectro-temporal features. This negative result highlights the limitations of cepstral coefficients for deep learning models that thrive on spatial correlations and suggests that the domain gap between natural images and MFCC heatmaps is too large for effective transfer without more suitable input representations. The introduction of Focal Loss in Phase 2 (AudioCNNv2) further improved performance, demonstrating the importance of addressing the severe class imbalance (608:1).

The most significant leap occurred in Phase 3 with the adoption of the Audio Spectrogram Transformer (AST) and Log-Mel Spectrograms (LMS). This combination achieved 57.28\% accuracy, a relative improvement of nearly 35\% over the baseline. This confirms that (1) LMS provides a superior representation for vision-based architectures applied to audio, and (2) the attention mechanism of AST effectively captures the temporal dynamics of bird calls better than fixed-window CNNs.

\subsection{Future Work: Multimodal Fusion}
Given the strong performance of our separate modalities, this work establishes a solid foundation for future multimodal integration. Our comparative analysis demonstrates that while vision is dominant, audio provides a distinct and complementary signal. Future work will focus on an \textit{Intermediate Feature Fusion} strategy. Rather than simple late fusion (averaging predictions), we aim to concatenate the embedding vectors from the penultimate layers of the fine-tuned ViT and AST models. These concatenated vectors will be fed into a joint Multi-Layer Perceptron (MLP) \cite{rosenblatt1958perceptron} to learn correlations between visual features (plumage, shape) and acoustic features (call structure), potentially resolving cases where one modality is ambiguous.

\section{Discussion}
\label{discussion}

Our results highlight the efficacy of Transformer-based architectures for fine-grained bird classification in both visual and acoustic modalities. The Vision Transformer (ViT) achieved the highest overall accuracy (92.33\% on the intersection dataset), outperforming the ResNet-18 baseline (85.52\%). This suggests that the self-attention mechanism in ViT is particularly well-suited for capturing the subtle, non-local discriminative features (such as plumage patterns and beak shapes) that distinguish similar bird species, even with a relatively small dataset like CUB-200. The strong performance of ViT, despite the lack of inductive bias typical of CNNs, can be attributed to the effective transfer learning from the large-scale ImageNet-21k pretraining.

In the audio domain, the transition from MFCCs to Log-Mel Spectrograms (LMS) proved critical. MFCCs, while standard for speech, discard too much spectro-temporal information, limiting the performance of deep models (37.79\% accuracy). By treating audio as an image via LMS, we enabled the use of powerful vision architectures. The Audio Spectrogram Transformer (AST) further capitalized on this by achieving 57.28\% accuracy, a massive improvement over the CNN baselines. This indicates that bird calls, which often have complex temporal structures and long-range dependencies (e.g., repeated motifs), benefit significantly from the global receptive field of Transformers.

We also observed that addressing class imbalance was essential. The use of Focal Loss in Phase 2 provided a clear performance boost, validating its utility in biodiversity datasets where species distributions are naturally long-tailed.

\paragraph{Training Dynamics and Generalization}
A critical analysis of the training curves revealed distinct behaviors between modalities (Table \ref{tab:training_dynamics}). The image-based Vision Transformer exhibited robust generalization, with training accuracy reaching 97\% and test accuracy maintaining 92\%, indicating that the large-scale pretraining on ImageNet-21k effectively bridged the domain gap. In contrast, all audio models suffered from significant overfitting due to the smaller size of the intersection dataset. The AudioViT baseline memorized noise early (epoch 4), while the AudioCNNv2 showed optimization instability with diverging validation loss after epoch 10. The AST model, despite achieving the best audio performance, also displayed a disconnect between loss and accuracy: validation loss began to rise after epoch 3, even as accuracy continued to improve until epoch 10. Similar overfitting on noisy, crowd-sourced audio data has been observed in other fine-grained audiovisual studies \cite{vanhorn2022}. This suggests that while Transformers are powerful, they require careful regularization or larger datasets to fully mitigate overfitting in the acoustic domain.

\begin{table}[ht]
\centering
\caption{Training Dynamics Analysis: Comparison of peak training accuracy, validation accuracy, and the epoch where validation loss began to diverge (overfitting onset).}
\label{tab:training_dynamics}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Modality} & \textbf{Train Acc} & \textbf{Val Acc} & \textbf{Overfitting Onset} \\
\midrule
ImageViT & Visual & 97\% & 92\% & None (Stable) \\
AudioViT & Audio & 99\% & 38\% & Epoch 4 \\
AudioCNNv2 & Audio & 95\% & 43\% & Epoch 10 \\
AST & Audio & 98\% & 57\% & Epoch 3 \\
\bottomrule
\end{tabular}%
}
\end{table}

\section{Conclusion}
\label{conclusion}

This work presented a comprehensive evaluation of deep learning methodologies for fine-grained audiovisual categorization of birds. We demonstrated that:
1.  **Transformers dominate:** ViT and AST consistently outperformed their convolutional counterparts in both image and audio tasks.
2.  **Representation matters:** Log-Mel Spectrograms are superior to MFCCs for deep learning-based audio classification.
3.  **Imbalance handling is key:** Techniques like Focal Loss are necessary to handle real-world biodiversity data distributions.

While image classification remains the more accurate modality (92.33\%), our audio models showed significant promise (57.28\%), suggesting that acoustic data contains complementary information. Future work will focus on the proposed multimodal fusion strategy to leverage the strengths of both domains, aiming for a robust, holistic bird identification system.

% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

If a paper is accepted, the final camera-ready version can (and
usually should) include acknowledgements.  Such acknowledgements
should be placed at the end of the section, in an unnumbered section
that does not count towards the paper page limit. Typically, this will 
include thanks to reviewers who gave useful comments, to colleagues 
who contributed to the ideas, and to funding agencies and corporate 
sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.''

% Se acharem melhor podem colocar isso aqui
% This work explores fine-grained bird classification from audio and images. The positive impacts include support biodiversity monitoring and aiding conservation research. Potential risks include misuse of solutions to find endangered species. We expected the benefits for the environment to outcome the potential risks.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
